{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(alpha, X, kernel, x):\n",
    "    '''\n",
    "     alpha:  vector of shape (n,) where n is the number of samples\n",
    "     X:      matrix of shape (n, 2) \n",
    "     kernel: a kernel function\n",
    "     x:      vector of shape (2,)\n",
    "\n",
    "    returns:\n",
    "     the result of evaluating f_w(x)\n",
    "    '''\n",
    "    K=[]\n",
    "    for i in range(len(X)):\n",
    "        K.append(kernel(X[i],x))\n",
    "    K=np.array(K)\n",
    "    result = np.dot(alpha,K)\n",
    "    return result\n",
    "\n",
    "def loss(alpha, X, kernel, x, y):\n",
    "    '''\n",
    "     alpha:  vector of shape (n,) where n is the number of samples\n",
    "     X:      matrix of shape (n, 2), training input samples\n",
    "     kernel: a kernel function\n",
    "     x:      vector of shape (2,), input sample \n",
    "     y:      scalar, target output sample\n",
    "    returns:\n",
    "     the result of evaluating the loss function for a sample (x, y)\n",
    "    '''\n",
    "    #loss function for all elements \n",
    "    E=np.abs(y-predict(alpha,X,kernel,x))\n",
    "    return  E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_de_dalpha(alpha, X, kernel, x, y, epsilon):\n",
    "    h = 1E-7\n",
    "    return (loss(alpha+h, X, kernel, x, y) - loss(alpha-h, X, kernel, x, y))/(2.0*h) \n",
    "            # return the vector of the derivatives\n",
    " # Hint use the central difference to compute the derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k1(x, y):\n",
    "    return np.dot(x, y)\n",
    "\n",
    "def k2(x, y):\n",
    "    return (np.dot(x, y) + 1) ** 2\n",
    "\n",
    "def de_dalpha(alpha, X, kernel, x, y):\n",
    "    K=[]\n",
    "    for i in range(len(X)):\n",
    "        K.append(kernel(X[i],x))\n",
    "    K=np.array(K)\n",
    "    de = -np.sign(y-predict(alpha, X, kernel, x))*K\n",
    "    return de\n",
    "\n",
    "def test_de_dalpha(kernel):\n",
    "    num_tests = 100\n",
    "    epsilon = 0.0001\n",
    "    X = [[-2, -1],\n",
    "         [-1, 3],\n",
    "         [2.5, -1.5],\n",
    "         [4, 2]]\n",
    "    for i in range(num_tests):\n",
    "        talpha = np.random.randn(len(X))\n",
    "        tx = np.random.randn(2)\n",
    "        ty = np.random.randn(1)\n",
    "        if np.linalg.norm(de_dalpha(talpha, X, kernel, tx, ty) - \n",
    "                          num_de_dalpha(talpha, X, kernel, tx, ty, epsilon)) > epsilon:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def test2():\n",
    "    return test_de_dalpha(k1) and test_de_dalpha(k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, Y, epochs, eta, alpha_ini, kernel):\n",
    "    '''\n",
    "     X:      matrix of shape (n, 2), training input samples\n",
    "     Y:      vector of shape (n, ), training output samples\n",
    "     epochs: number of epochs\n",
    "     eta:    learning rate\n",
    "     alpha_ini:  vector of shape (n,), initial values of alpha\n",
    "     kernel: a kernel function\n",
    "    returns:\n",
    "     a tuple (alpha, losses) where:\n",
    "       alpha: vector of shape (n, ) with resulting alpha values\n",
    "       losses: a vector of shape (epochs, ) with the loss values for each epoch\n",
    "    '''\n",
    "    losses = []\n",
    "    alpha = alpha_ini\n",
    "    for epoch in range(epochs):\n",
    "        loss_tmp=0\n",
    "        for j in range(len(X)):\n",
    "            #forward propagation\n",
    "            prediction=predict(alpha,X,kernel,X[j]) # Here is the prediction function we show at the begining\n",
    "            #Back propagation\n",
    "            alpha-=  num_de_dalpha(alpha, X, kernel, X[j], Y[j], epsilon)*eta - alpha*num_de_dalpha(alpha, X, kernel, X[j], Y[j], epsilon) # Here we compute the derivatives times the learning rate and we subtract the value of alpha times its derivative\n",
    "        for j in range(len(x)):\n",
    "            loss_tmp+=loss(alpha, X, kernel, X[j], Y[j])\n",
    "        losses.append(loss_tmp)   \n",
    "    return alpha, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [[-2, -1],\n",
    "     [-1, 3],\n",
    "     [2.5, -1.5],\n",
    "     [4, 2]]\n",
    "Y = [1, 1, 1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
